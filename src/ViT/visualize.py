"""
ViT å¯è§†åŒ–å·¥å…·
åŠŸèƒ½ï¼š
1. å¯è§†åŒ–è®­ç»ƒæ—¥å¿—ä¸­çš„losså’Œå‡†ç¡®ç‡æ›²çº¿
2. å¯è§†åŒ–æŒ‡å®šå›¾åƒçš„attention map
"""

import os
import re
import argparse
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
from torchvision import transforms
from .modules.ViT_model import create_vit_base_patch16


def parse_log_file(log_path):
    """
    è§£æè®­ç»ƒæ—¥å¿—æ–‡ä»¶
    
    Args:
        log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: åŒ…å«è®­ç»ƒå’ŒéªŒè¯æ•°æ®çš„å­—å…¸
    """
    epochs = []
    train_loss = []
    train_acc = []
    val_loss = []
    val_acc = []
    
    with open(log_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ•°æ®
    # åŒ¹é…æ ¼å¼: Epoch 31/260 [Stage 2]
    # Train - Loss: 4.8913, Acc@1: 5.03%, Acc@5: 16.47%
    # Val   - Loss: 4.5237, Acc@1: 9.38%, Acc@5: 25.67%
    
    pattern = r'Epoch (\d+)/\d+.*?\nTrain - Loss: ([\d.]+), Acc@1: ([\d.]+)%.*?\nVal   - Loss: ([\d.]+), Acc@1: ([\d.]+)%'
    matches = re.findall(pattern, content)
    
    for match in matches:
        epoch, t_loss, t_acc, v_loss, v_acc = match
        epochs.append(int(epoch))
        train_loss.append(float(t_loss))
        train_acc.append(float(t_acc))
        val_loss.append(float(v_loss))
        val_acc.append(float(v_acc))
    
    return {
        'epochs': epochs,
        'train_loss': train_loss,
        'train_acc': train_acc,
        'val_loss': val_loss,
        'val_acc': val_acc
    }


def plot_training_curves(log_path, save_dir=None):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    
    Args:
        log_path: è®­ç»ƒæ—¥å¿—è·¯å¾„
        save_dir: ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¿å­˜åˆ°æ—¥å¿—åŒç›®å½•
    """
    print(f"ğŸ“Š è§£æè®­ç»ƒæ—¥å¿—: {log_path}")
    data = parse_log_file(log_path)
    
    if len(data['epochs']) == 0:
        print("âŒ æœªèƒ½ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–åˆ°æ•°æ®")
        return
    
    print(f"âœ“ æˆåŠŸæå– {len(data['epochs'])} ä¸ªepochçš„æ•°æ®")
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # ç»˜åˆ¶Lossæ›²çº¿
    axes[0].plot(data['epochs'], data['train_loss'], label='Train Loss', linewidth=2)
    axes[0].plot(data['epochs'], data['val_loss'], label='Val Loss', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].set_title('Loss Curve', fontsize=14, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ç»˜åˆ¶Accuracyæ›²çº¿
    axes[1].plot(data['epochs'], data['train_acc'], label='Train Acc@1', linewidth=2)
    axes[1].plot(data['epochs'], data['val_acc'], label='Val Acc@1', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Accuracy (%)', fontsize=12)
    axes[1].set_title('Accuracy Curve', fontsize=14, fontweight='bold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    if save_dir is None:
        save_dir = os.path.dirname(log_path)
    
    save_path = os.path.join(save_dir, 'training_curves.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
    print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(data['val_acc']):.2f}% (Epoch {data['epochs'][np.argmax(data['val_acc'])]})")
    print(f"  æœ€ä½éªŒè¯Loss: {min(data['val_loss']):.4f} (Epoch {data['epochs'][np.argmin(data['val_loss'])]})")
    print(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {data['val_acc'][-1]:.2f}%")


def visualize_attention(image_path, model_path, layer_idx=-1, save_dir=None):
    """
    å¯è§†åŒ–æŒ‡å®šå›¾åƒçš„attention map
    
    Args:
        image_path: è¾“å…¥å›¾åƒè·¯å¾„
        model_path: æ¨¡å‹æƒé‡è·¯å¾„
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        layer_idx: Transformerå±‚ç´¢å¼•ï¼Œ-1è¡¨ç¤ºæœ€åä¸€å±‚
        save_dir: ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºresult_dir
    """
    print(f"\nğŸ” å¯è§†åŒ–Attention Map")
    print(f"  å›¾åƒ: {image_path}")
    print(f"  æ¨¡å‹: {model_path}")
    print(f"  Layer: {layer_idx}")
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = create_vit_base_patch16(config=None) # åˆ›å»ºé»˜è®¤çš„ç©ºæ¨¡å‹
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location=device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.to(device)
    model.eval()
    print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    img = Image.open(image_path).convert('RGB')
    img_tensor = transform(img).unsqueeze(0).to(device)
    
    # è·å–attention maps
    with torch.no_grad():
        attention_maps = model.get_attention_maps(img_tensor, layer_idx=layer_idx)
    
    if attention_maps is None:
        print("âŒ æ— æ³•è·å–attention maps")
        return
    
    # attention_maps shape: (1, num_heads, num_patches+1, num_patches+1)
    attention_maps = attention_maps[0].cpu().numpy()  # (num_heads, 197, 197)
    num_heads = attention_maps.shape[0]
    
    # æå–CLS tokenå¯¹æ‰€æœ‰patchçš„attention (ç¬¬ä¸€è¡Œï¼Œè·³è¿‡CLSè‡ªå·±)
    cls_attention = attention_maps[:, 0, 1:]  # (num_heads, 196)
    
    # è®¡ç®—å¹³å‡attention
    avg_attention = cls_attention.mean(axis=0)  # (196,)
    
    # Reshapeåˆ°2D grid
    num_patches = int(np.sqrt(cls_attention.shape[1]))
    cls_attention_2d = cls_attention.reshape(num_heads, num_patches, num_patches)
    avg_attention_2d = avg_attention.reshape(num_patches, num_patches)
    
    # åˆ›å»ºå¯è§†åŒ–
    fig = plt.figure(figsize=(16, 10))
    
    # æ˜¾ç¤ºåŸå›¾
    ax = plt.subplot(3, 5, 1)
    ax.imshow(img)
    ax.set_title('Original Image', fontsize=12, fontweight='bold')
    ax.axis('off')
    
    # æ˜¾ç¤ºå¹³å‡attention
    ax = plt.subplot(3, 5, 2)
    im = ax.imshow(avg_attention_2d, cmap='jet', interpolation='bilinear')
    ax.set_title('Average Attention', fontsize=12, fontweight='bold')
    ax.axis('off')
    plt.colorbar(im, ax=ax, fraction=0.046)
    
    # æ˜¾ç¤ºå åŠ åçš„å›¾åƒ
    ax = plt.subplot(3, 5, 3)
    img_resized = np.array(img.resize((num_patches * 16, num_patches * 16)))
    attention_upsampled = np.array(Image.fromarray(
        (avg_attention_2d * 255).astype(np.uint8)
    ).resize((num_patches * 16, num_patches * 16), Image.BILINEAR))
    attention_upsampled = attention_upsampled / 255.0
    
    ax.imshow(img_resized)
    ax.imshow(attention_upsampled, cmap='jet', alpha=0.5, interpolation='bilinear')
    ax.set_title('Attention Overlay', fontsize=12, fontweight='bold')
    ax.axis('off')
    
    # æ˜¾ç¤ºå„ä¸ªheadçš„attention
    for i in range(min(12, num_heads)):
        ax = plt.subplot(3, 5, i + 4)
        im = ax.imshow(cls_attention_2d[i], cmap='jet', interpolation='bilinear')
        ax.set_title(f'Head {i+1}', fontsize=10)
        ax.axis('off')
    
    layer_name = f"Layer {layer_idx}" if layer_idx >= 0 else f"Layer {model.depth + layer_idx}"
    plt.suptitle(f'Attention Visualization - {layer_name}', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # ä¿å­˜
    if save_dir is None:
        save_dir = './result/vit/vis_images'
    
    os.makedirs(save_dir, exist_ok=True)
    img_name = os.path.splitext(os.path.basename(image_path))[0]
    save_path = os.path.join(save_dir, f'attention_{img_name}_layer{layer_idx}.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ Attentionå¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")


def main():
    parser = argparse.ArgumentParser(description='ViT å¯è§†åŒ–å·¥å…·')
    parser.add_argument('--mode', type=str, required=True, choices=['curves', 'attention'],
                        help='å¯è§†åŒ–æ¨¡å¼: curves(è®­ç»ƒæ›²çº¿) æˆ– attention(æ³¨æ„åŠ›å›¾)')
    
    # è®­ç»ƒæ›²çº¿å‚æ•°
    parser.add_argument('--log', type=str, help='è®­ç»ƒæ—¥å¿—æ–‡ä»¶è·¯å¾„')
    
    # Attentionå¯è§†åŒ–å‚æ•°
    parser.add_argument('--image', type=str, help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--model', type=str, help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--layer', type=int, default=-1, help='Transformerå±‚ç´¢å¼• (-1è¡¨ç¤ºæœ€åä¸€å±‚)')
    
    # é€šç”¨å‚æ•°
    parser.add_argument('--save_dir', type=str, default=None, help='ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    if args.mode == 'curves':
        if not args.log:
            print("âŒ è¯·æŒ‡å®šè®­ç»ƒæ—¥å¿—æ–‡ä»¶ (--log)")
            return
        if not os.path.exists(args.log):
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {args.log}")
            return
        plot_training_curves(args.log, args.save_dir)
        
    elif args.mode == 'attention':
        if not args.image or not args.model:
            print("âŒ è¯·æŒ‡å®šå›¾åƒè·¯å¾„ (--image) å’Œæ¨¡å‹è·¯å¾„ (--model)")
            return
        if not os.path.exists(args.image):
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {args.image}")
            return
        if not os.path.exists(args.model):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
            return
        visualize_attention(args.image, args.model, args.layer, args.save_dir)


if __name__ == '__main__':
    main()
